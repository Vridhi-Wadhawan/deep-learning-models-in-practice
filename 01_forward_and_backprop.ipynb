{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vridhi-Wadhawan/deep-learning-models-in-practice/blob/main/01_forward_and_backprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Forward & Backpropagation"
      ],
      "metadata": {
        "id": "kr107MmiZPAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "This notebook implements forward propagation and backpropagation from scratch for a simple 2â€“2â€“1 neural network.\n",
        "\n",
        "What this notebook demonstrates:\n",
        "- Xavier initialization\n",
        "- Manual forward propagation\n",
        "- Closed-form backpropagation gradients\n",
        "- One-step gradient descent\n",
        "- Effect of ReLU vs linear activation\n"
      ],
      "metadata": {
        "id": "Gi2K7mmUO8xt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Setup"
      ],
      "metadata": {
        "id": "5682rKTdCMda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Libraries\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "_MfcWIAha2D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAFPtPHeW5U8",
        "outputId": "85969c61-2530-4108-957c-7cc40da6348e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i1 =  2\n",
            "i2 =  4\n"
          ]
        }
      ],
      "source": [
        "# Inputs\n",
        "i1 = 2\n",
        "i2 = 4\n",
        "print(\"i1 = \", i1)\n",
        "print(\"i2 = \", i2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting random seed for reproducibility\n",
        "np.random.seed(38)"
      ],
      "metadata": {
        "id": "EoxSz0y_a7XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Xavier Initialization & Forward Propagation Using Linear Activation Function On Hidden Layers\n"
      ],
      "metadata": {
        "id": "D4jkzqCkZRct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why Xavier Initialization?**\n",
        "- Prevents exploding/vanishing activations\n",
        "- Keeps signal variance stable across layers\n",
        "\n",
        "For a layer with ð‘› inputs and ð‘› outputs, weights are drawn from:\n",
        "\n",
        "**ð‘Šâˆ¼ð‘ˆ(âˆ’ sqrt.(6/ð‘› input + ð‘› output), sqrt.(6/ð‘› input + ð‘› output))**\n"
      ],
      "metadata": {
        "id": "kCQlExxxGL6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Xavier initialization bounds\n",
        "bound_input_hidden = np.sqrt(6 / (2 + 2))   # for W1, W2, W3 & W4\n",
        "bound_hidden_output = np.sqrt(6 / (2 + 1)) # for W5 & W6"
      ],
      "metadata": {
        "id": "qyI9bub0bDeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights\n",
        "W1, W2, W3, W4 = np.random.uniform(-bound_input_hidden, bound_input_hidden, 4)\n",
        "W5, W6 = np.random.uniform(-bound_hidden_output, bound_hidden_output, 2)\n",
        "\n",
        "initialized_weights = [W1, W2, W3, W4, W5, W6]\n",
        "\n",
        "print(\"Initialized Weights:\")\n",
        "for idx, w in enumerate(initialized_weights, start=1):\n",
        "    print(f\"W{idx}: {w}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i6amOrDbIAy",
        "outputId": "8dfe26cb-68ed-4076-80d2-aa1098ba68b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized Weights:\n",
            "W1: -0.2822470618366393\n",
            "W2: 0.8811006806223465\n",
            "W3: 1.0880624516582986\n",
            "W4: 0.49681749648057094\n",
            "W5: 0.3779743052305753\n",
            "W6: 0.2997037641550875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward propagation\n",
        "h1 = W1 * i1 + W2 * i2 # hidden layer\n",
        "h2 = W3 * i1 + W4 * i2 # hidden layer\n",
        "out = W5 * h1 + W6 * h2 # output layer\n",
        "\n",
        "print(\"\\nForward Pass Results:\")\n",
        "print(f\"h1 = {h1}, h2 = {h2}\")\n",
        "print(f\"Output = {out}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKKVHj5XbKKw",
        "outputId": "355e8aeb-7716-4c69-d0c9-3225fbefc3f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Forward Pass Results:\n",
            "h1 = 2.9599085988161074, h2 = 4.163394889238881\n",
            "Output = 2.36655451615247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward Propagation (Linear Activation)**\n",
        "\n",
        "Hidden units compute weighted sums of inputs.\n",
        "No non-linearity is applied at the hidden layer."
      ],
      "metadata": {
        "id": "hf-VkjyYJGyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check: manual vs computed forward pass\n",
        "assert np.isclose(h1, W1 * i1 + W2 * i2)\n",
        "assert np.isclose(h2, W3 * i1 + W4 * i2)"
      ],
      "metadata": {
        "id": "BqFrs_eASuMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xavier initialization enables stable signal propagation, while the linear forward pass applies pure weighted sums without non-linearity.\n"
      ],
      "metadata": {
        "id": "DkfIp0lzMidU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation Gradients (Closed Form)"
      ],
      "metadata": {
        "id": "WQvgXTUTfRB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Closed-form gradients are derived analytically and implemented directly below.\n",
        "\n",
        "Final Gradient Set:\n",
        "\n",
        "*Output Layer*\n",
        "- âˆ‚ð¿/âˆ‚ð‘Š5 = (out âˆ’ ð‘¦).(â„Ž1)\n",
        "- âˆ‚ð¿/âˆ‚ð‘Š6 = (out âˆ’ ð‘¦).(â„Ž2)\n",
        "\n",
        "*Hidden Layer*\n",
        "- âˆ‚ð¿/âˆ‚ð‘Š1 = (out âˆ’ ð‘¦).(ð‘Š5).(ð‘–1)\n",
        "- âˆ‚ð¿/âˆ‚ð‘Š2 = (out âˆ’ ð‘¦).(ð‘Š5).(ð‘–2)\n",
        "- âˆ‚ð¿/âˆ‚ð‘Š3 = (out âˆ’ ð‘¦).(ð‘Š6).(ð‘–1)\n",
        "- âˆ‚ð¿/âˆ‚ð‘Š4 = (out âˆ’ ð‘¦).(ð‘Š6).(ð‘–2)"
      ],
      "metadata": {
        "id": "iIN4KxBbp_KG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One-Step Gradient Descent Update"
      ],
      "metadata": {
        "id": "v7Pe13FXfnd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters:\n",
        "- target ð‘¦ = 1\n",
        "- learning rate ðœ‚ = 0.05"
      ],
      "metadata": {
        "id": "wv8Vps7OmFNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate and target (given)\n",
        "eta = 0.05\n",
        "y = 1"
      ],
      "metadata": {
        "id": "V5RsAS3vgCub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recomputing forward pass to check if the values are correct\n",
        "h1 = W1 * i1 + W2 * i2\n",
        "h2 = W3 * i1 + W4 * i2\n",
        "out = W5 * h1 + W6 * h2\n",
        "\n",
        "print(\"\\nForward Pass Results:\")\n",
        "print(f\"h1 = {h1}, h2 = {h2}\")\n",
        "print(f\"Output = {out}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5w_zFL8gfMH",
        "outputId": "86d5208e-594a-450d-ba1c-df7292ce984d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Forward Pass Results:\n",
            "h1 = 2.9599085988161074, h2 = 4.163394889238881\n",
            "Output = 2.36655451615247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Function**\n",
        "\n",
        "Mean Squared Error (MSE):   L = Â½ (y âˆ’ Å·)Â²"
      ],
      "metadata": {
        "id": "FWw-UV79V0uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss gradient with respect to output\n",
        "dL_dout = out - y\n",
        "print(f\"Differentiating the loss w.r.t. output (dL/dout) = {dL_dout}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaYxw5-YiqYH",
        "outputId": "a86b6098-84aa-4ee9-f3b1-abfe4483b5b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Differentiating the loss w.r.t. output (dL/dout) = 1.36655451615247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradients\n",
        "dL_dW5 = dL_dout * h1\n",
        "dL_dW6 = dL_dout * h2\n",
        "\n",
        "dL_dh1 = dL_dout * W5\n",
        "dL_dh2 = dL_dout * W6\n",
        "\n",
        "dL_dW1 = dL_dh1 * i1\n",
        "dL_dW2 = dL_dh1 * i2\n",
        "dL_dW3 = dL_dh2 * i1\n",
        "dL_dW4 = dL_dh2 * i2\n",
        "\n",
        "# Collect gradients\n",
        "grads = [dL_dW1, dL_dW2, dL_dW3, dL_dW4, dL_dW5, dL_dW6]\n",
        "\n",
        "print(\"Gradients before update:\")\n",
        "for idx, g in enumerate(grads, start=1):\n",
        "    print(f\"dL/dW{idx} = {g}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3fS1k9aizYr",
        "outputId": "3c4c018d-2bda-4879-def0-3dbb850305b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients before update:\n",
            "dL/dW1 = 1.0330449876048695\n",
            "dL/dW2 = 2.066089975209739\n",
            "dL/dW3 = 0.8191230648280591\n",
            "dL/dW4 = 1.6382461296561182\n",
            "dL/dW5 = 4.044876463110681\n",
            "dL/dW6 = 5.689506088415505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Updated rule**\n",
        "- For any weight ð‘Š:\n",
        "\n",
        "ð‘Š(new) = ð‘Š(old) âˆ’ (ðœ‚)â‹…(âˆ‚ð¿/âˆ‚ð‘Š)"
      ],
      "metadata": {
        "id": "85eNjn9hnU4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update weights\n",
        "W1_new = W1 - eta * dL_dW1\n",
        "W2_new = W2 - eta * dL_dW2\n",
        "W3_new = W3 - eta * dL_dW3\n",
        "W4_new = W4 - eta * dL_dW4\n",
        "W5_new = W5 - eta * dL_dW5\n",
        "W6_new = W6 - eta * dL_dW6\n",
        "\n",
        "# Collect updated weights\n",
        "updated_weights = [W1_new, W2_new, W3_new, W4_new, W5_new, W6_new]\n",
        "\n",
        "print(\"\\nUpdated Weights:\")\n",
        "for idx, w in enumerate(updated_weights, start=1):\n",
        "    print(f\"W{idx}: {w}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yH4sC1Ppi37H",
        "outputId": "0b74a023-27f4-4cda-89f6-6d7e5c7a955d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Updated Weights:\n",
            "W1: -0.33389931121688277\n",
            "W2: 0.7777961818618595\n",
            "W3: 1.0471062984168957\n",
            "W4: 0.41490518999776504\n",
            "W5: 0.17573048207504124\n",
            "W6: 0.01522845973431225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recompute forward pass with updated weights\n",
        "h1_new = W1_new * i1 + W2_new * i2\n",
        "h2_new = W3_new * i1 + W4_new * i2\n",
        "out_new = W5_new * h1_new + W6_new * h2_new\n",
        "\n",
        "# Compute new loss\n",
        "loss_new = 0.5 * (y - out_new) ** 2\n",
        "\n",
        "print(\"\\nAfter Weight Update:\")\n",
        "print(f\"h1_new = {h1_new}\")\n",
        "print(f\"h2_new = {h2_new}\")\n",
        "print(f\"Output_new = {out_new}\")\n",
        "print(f\"New Loss = {loss_new}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpFErwQvmqsd",
        "outputId": "84104116-83a7-4e55-8d4c-ab92ddaeb1e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After Weight Update:\n",
            "h1_new = 2.4433861050136727\n",
            "h2_new = 3.7538333568248516\n",
            "Output_new = 0.4865425182532355\n",
            "New Loss = 0.13181929278086452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After one gradient descent step, the loss decreases, confirming that the network learns in the correct direction."
      ],
      "metadata": {
        "id": "rqnBcrsXpkWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward Propagation with ReLU Activation Function on Hidden Layers"
      ],
      "metadata": {
        "id": "nANPma72lG4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using same inputs and initial weights from previous Questions\n",
        "\n",
        "# Pre-activations for hidden layer\n",
        "z1 = W1 * i1 + W2 * i2\n",
        "z2 = W3 * i1 + W4 * i2\n",
        "\n",
        "# ReLU activations\n",
        "h1 = np.maximum(0, z1)\n",
        "h2 = np.maximum(0, z2)\n",
        "\n",
        "# Output layer (linear activation)\n",
        "out = W5 * h1 + W6 * h2\n",
        "\n",
        "print(\"With ReLU on hidden units:\")\n",
        "print(f\"z1 = {z1}, z2 = {z2}\")\n",
        "print(f\"h1 (ReLU) = {h1}, h2 (ReLU) = {h2}\")\n",
        "print(f\"Output = {out}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5GhfvGopu2Y",
        "outputId": "006ea1d1-7f26-456d-bbbc-f7f8e7d09476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With ReLU on hidden units:\n",
            "z1 = 2.9599085988161074, z2 = 4.163394889238881\n",
            "h1 (ReLU) = 2.9599085988161074, h2 (ReLU) = 4.163394889238881\n",
            "Output = 2.36655451615247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since both z1 and z2 are positive, ReLU behaves identically to the linear activation in this specific example. If either one was not positive, then neuronâ€™s activation becomes zero, removing its contribution to the output."
      ],
      "metadata": {
        "id": "3HxY5uN7FzGS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear vs ReLU: Key Observation**\n",
        "\n",
        "- Linear activation always propagates gradients\n",
        "- ReLU blocks gradients when pre-activation â‰¤ 0\n",
        "- In this example, both neurons remain active, so updates match the linear case\n"
      ],
      "metadata": {
        "id": "tw4MN2ijUHt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weight Updates with ReLU in Hidden Layer"
      ],
      "metadata": {
        "id": "hbCyHNHQqku7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using same inputs and initial weights from previous Questions\n",
        "# Forward pass (with ReLU on hidden)\n",
        "z1 = W1 * i1 + W2 * i2\n",
        "z2 = W3 * i1 + W4 * i2\n",
        "\n",
        "h1 = np.maximum(0, z1)\n",
        "h2 = np.maximum(0, z2)\n",
        "\n",
        "out = W5 * h1 + W6 * h2"
      ],
      "metadata": {
        "id": "iDj4oIvkr9KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute loss\n",
        "loss = 0.5 * (y - out) ** 2\n",
        "print(f\"Loss before update = {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWsf470PtFfL",
        "outputId": "c2482a5b-813a-4907-ad1f-5f3963b3cea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss before update = 0.9337356228083555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagation\n",
        "dL_dout = out - y\n",
        "print(f\"Differentiating the loss w.r.t. output (dL/dout) = {dL_dout}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGMdySvXtJRU",
        "outputId": "784f7d1b-8d5d-4444-c79a-ec89b2a41a39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Differentiating the loss w.r.t. output (dL/dout) = 1.36655451615247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradients for W5, W6\n",
        "dL_dW5 = dL_dout * h1\n",
        "dL_dW6 = dL_dout * h2"
      ],
      "metadata": {
        "id": "BuPxVm5ntU2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Backprop to hidden layer (include ReLU derivative)\n",
        "relu_grad_h1 = 1.0 if z1 > 0 else 0.0\n",
        "relu_grad_h2 = 1.0 if z2 > 0 else 0.0\n",
        "\n",
        "dL_dh1 = dL_dout * W5 * relu_grad_h1\n",
        "dL_dh2 = dL_dout * W6 * relu_grad_h2\n",
        "\n",
        "# Gradients for W1, W2, W3 & W4\n",
        "dL_dW1 = dL_dh1 * i1\n",
        "dL_dW2 = dL_dh1 * i2\n",
        "dL_dW3 = dL_dh2 * i1\n",
        "dL_dW4 = dL_dh2 * i2\n",
        "\n",
        "# Collect gradients\n",
        "grads = [dL_dW1, dL_dW2, dL_dW3, dL_dW4, dL_dW5, dL_dW6]\n",
        "\n",
        "print(\"Gradients before update:\")\n",
        "for idx, g in enumerate(grads, start=1):\n",
        "    print(f\"dL/dW{idx} = {g}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqXPfFKAvY7Z",
        "outputId": "20c9d49f-e409-41d6-9f05-afc09b7d78b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients before update:\n",
            "dL/dW1 = 1.0330449876048695\n",
            "dL/dW2 = 2.066089975209739\n",
            "dL/dW3 = 0.8191230648280591\n",
            "dL/dW4 = 1.6382461296561182\n",
            "dL/dW5 = 4.044876463110681\n",
            "dL/dW6 = 5.689506088415505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update weights\n",
        "W1_new = W1 - eta * dL_dW1\n",
        "W2_new = W2 - eta * dL_dW2\n",
        "W3_new = W3 - eta * dL_dW3\n",
        "W4_new = W4 - eta * dL_dW4\n",
        "W5_new = W5 - eta * dL_dW5\n",
        "W6_new = W6 - eta * dL_dW6\n",
        "\n",
        "# Collect updated weights\n",
        "updated_weights = [W1_new, W2_new, W3_new, W4_new, W5_new, W6_new]\n",
        "\n",
        "print(\"\\nUpdated Weights:\")\n",
        "for idx, w in enumerate(updated_weights, start=1):\n",
        "    print(f\"W{idx}: {w}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkbRU7cdtbwv",
        "outputId": "e46bf399-8c66-43f3-97e4-0bd7baa88452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Updated Weights:\n",
            "W1: -0.33389931121688277\n",
            "W2: 0.7777961818618595\n",
            "W3: 1.0471062984168957\n",
            "W4: 0.41490518999776504\n",
            "W5: 0.17573048207504124\n",
            "W6: 0.01522845973431225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass again with updated weights\n",
        "h1_new = np.maximum(0, W1_new * i1 + W2_new * i2)\n",
        "h2_new = np.maximum(0, W3_new * i1 + W4_new * i2)\n",
        "out_new = W5_new * h1_new + W6_new * h2_new\n",
        "\n",
        "loss_new = 0.5 * (y - out_new) ** 2\n",
        "\n",
        "print(\"\\nAfter Weight Update:\")\n",
        "print(f\"h1_new = {h1_new}\")\n",
        "print(f\"h2_new = {h2_new}\")\n",
        "print(f\"Output_new = {out_new}\")\n",
        "print(f\"New Loss = {loss_new}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGHTVdTpteiq",
        "outputId": "2d4ecc49-fae4-45ef-aa9c-5ab076352fde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After Weight Update:\n",
            "h1_new = 2.4433861050136727\n",
            "h2_new = 3.7538333568248516\n",
            "Output_new = 0.4865425182532355\n",
            "New Loss = 0.13181929278086452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since both z_1 and z_2 are positive, so ReLU doesnâ€™t change their values. This makes h_1, h_2, the output, and the gradients identical to the linear activation case shown earlier. The loss decreases after the update, confirming learning. If either z =< 0, that neuronâ€™s output would be 0 and its incoming weights would not update."
      ],
      "metadata": {
        "id": "YEKXkjHuh1q9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Takeaways\n",
        "\n",
        "- Backpropagation is a direct application of the chain rule\n",
        "- Activation functions control gradient flow\n",
        "- Weight initialization plays a critical role in stable learning\n",
        "\n",
        "## Limitations\n",
        "\n",
        "- Only a single training step is demonstrated\n",
        "- No batching or multi-epoch optimization\n",
        "- Network size is intentionally minimal for clarity"
      ],
      "metadata": {
        "id": "1X6tbiLdUX1A"
      }
    }
  ]
}